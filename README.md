# ðŸ©º Medical Chatbot using Generative AI  

ðŸš€ This project is an AI-powered **Medical Chatbot** that leverages **Retrieval-Augmented Generation (RAG)** to provide **context-aware** and **accurate** medical responses. It integrates **TinyLlama-1.1B-Chat** as the language model and **Pinecone** as the vector store for efficient retrieval.  

## ðŸ“Œ Features  
- âœ… **AI-Powered Medical Assistance** â€“ Provides informative responses to medical queries.  
- âœ… **Retrieval-Augmented Generation (RAG)** â€“ Fetches relevant medical information before generating responses.  
- âœ… **TinyLlama-1.1B-Chat LLM** â€“ Lightweight and efficient for conversational AI.  
- âœ… **Pinecone Vector Search** â€“ Fast and scalable similarity search for embeddings.  
- âœ… **Hugging Face Model API** â€“ Easily integrates models for inference.  
- âœ… **Flask Web Interface** â€“ Simple and interactive chatbot interface.  

## ðŸ›  Tech Stack  
| **Component**     | **Technology**                                      |
|------------------|--------------------------------------------------|
| **Backend**      | Flask, LangChain, Hugging Face API               |
| **LLM Model**    | TinyLlama-1.1B-Chat (via Hugging Face)           |
| **Vector DB**    | Pinecone                                         |
| **Embeddings**   | sentence-transformers/all-MiniLM-L6-v2           |
| **Frontend**     | HTML, CSS, JavaScript                            |

## ðŸ“– How It Works  
The chatbot follows a **Retrieval-Augmented Generation (RAG)** approach:  
1. **User Input** â€“ The chatbot receives a medical query.  
2. **Vector Search** â€“ Converts the query into an embedding and retrieves relevant documents from **Pinecone**.  
3. **LLM Processing** â€“ The **TinyLlama-1.1B-Chat** model generates an AI-powered response using the retrieved information.  
4. **Response Generation** â€“ The chatbot returns a well-informed medical response.  

---
